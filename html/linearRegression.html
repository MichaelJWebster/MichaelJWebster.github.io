<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other
	 head content must come *after* these tags -->
    <meta name="description" content="Linear Regression.">
    <meta name="author" content="Michael Webster.">
    <link rel="icon" href="../img/black-white-metro-globe-icon.png">
    <title>Linear Regression.</title>
    <!-- google code prettifier. -->
    <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js">
    </script>
    <!-- Bootstrap core CSS -->
    <link href="../bower_components/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Bootstrap theme -->
    <link href="../bower_components/bootstrap/dist/css/bootstrap-theme.min.css" rel="stylesheet">
    <!-- darkly themed bootstrap CSS -->
    <!-- <link href="../css/bootstrap.min.css" rel="stylesheet">-->
    <!-- Custom styles for this template -->
    <!--<link href="../css/theme.css" rel="stylesheet">-->
    <link href="../css/grendelHome.css" rel="stylesheet">
    <!-- Add mathjax for using latex. -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" }}
      });
    </script>
    <script type="text/javascript" async
	    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
      </script>
  </head>

  <body role="document">

    <!-- Fixed navbar -->
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Linear Regression</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="../index.html">Home</a></li>
            <li><a href="../html/about.html">..About</a></li>
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Dropdown <span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="#">Action</a></li>
                <li><a href="#">Another action</a></li>
                <li><a href="#">Something else here</a></li>
                <li role="separator" class="divider"></li>
                <li class="dropdown-header">Nav header</li>
                <li><a href="#">Separated link</a></li>
                <li><a href="#">One more separated link</a></li>
              </ul>
            </li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container-fluid" role="main">

      <!-- Main jumbotron for a primary marketing message or call to action -->
      <div class="row-fluid">
	<div class="col-md-11">
          <h1>Linear Regression</h1>
          <p>Linear Regression is a minimisation problem. We are given a set of
	    measurements of independent variables, and a dependent variable, and we
	    want to come up with a way of understanding or predicting the dependent variable
	    in terms of linear combinations of the input variables.</p>

	  <p>More formally, we're trying to come up with a set of parameter values \(\theta\)
	    for a hypothesis such that:</p>
	  <div class="well well-sm">
	    \begin{equation}
	    h_{\theta}(x^{(i)}) = \theta . X^{(i)T}
	    \end{equation}
	  </div>

	  <p>and the following cost function is minimised:</p>

	  <div class="well well-sm">
	  \begin{equation}
	  J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^{2}
	  \end{equation}
	  </div>

	  <p>The technique used to minimise equation (1) (in linear regression at least)
	    is called gradient descent. The algorithm proceeds by finding the gradient of
	    the cost function with respect to each value \(\theta_{j}\), and subtracting
	    some fraction of that gradient from the \(\theta_{j}\) to move the cost function
	    down the minimisation gradient.</p>

	  <h4>Gradient Descent Algorithm: V1</h4>

	  <p>Repeat:</p>
	  <div class="well well-sm">	  
	  \begin{equation}
	  \theta_{j} := \theta_{j} - \alpha \frac{\partial}{\partial \theta_{j}}
	  J(\theta_0, \dots ,\theta_n)
	  \end{equation}
	  </div>
	  <p>until some convergence test is satisfied, or we give up.</p>
	  <p><b>Note:</b> in equation (3) above, \(\alpha\) is called the learning rate. It is a tunable
	    paramter. If \(\alpha\) is too high, the regression will fail to converge. If \(\alpha\) is
	    low, the algorithm will take more iterations to find the minimum.</p>
	  <h4>Gradient Descent Algorithm with derivitives of the cost function:</h4>
	  <p><em>Repeat:</em>
	    <div class="well well-sm">
	    \begin{equation}
	    \theta_{j} = \theta_{j} - \alpha \frac{1}{m}
	    \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}
	    \end{equation}
	    </div>
	  <p>simultaneously update \(\theta_{j}\) for every \(j = 0, \dots ,n\).</p>

	  <h2>Example using Galton's height data</h2>
	  <p><a href="http://www.math.uah.edu/stat/data/Galton.html">Galton's height data</a> is
	    a dataset containing information about the height of parents and their children from
	    an 1885 study by <a href="http://www.math.uah.edu/stat/biographies/Galton.html">
	      Francis Galton</a>.
	</div> <!-- class="col-md-11" -->
      </div>
      <div class="container-fluid">
	<div class="row-fluid">
	  <div class="col-md-8">
	    <!-- text -->
	    <p>The original dataset contains the following columns:</p>
	    <ul>
	      <li>Family</li>
	      <li>Father's Height</li>
	      <li>Mother's Height</li>
	      <li>Gender</li>
	      <li>Height</li>
	      <li>Number of children in family.</li>
	    </ul>
	    <p>To perform a linear regression on this data set, it will be helpful to separate it
	      into two datasets, one for male child heights, and one for female child heights. Both
	      of these data sets should simply include the "Father", "Mother" and "Height" fields.
	    </p>
	    <p>
	      We can now perform linear regressions on the Male and Female data to determine
	      a predictor for height based on the heights of the parents.
	    </p>
	    <h3>Dividing our data into training, cross validation and test sets.</h3>
	    <p>For each of the male and female datasets, we can divide the sets into
	      <em>Train</em>, <em>Cross Validation</em>, and <em>Test</em> sets.
	      Currently for the male data set we have our data in an array of objects where
	      each object contains values for the Father, Mother and Height fields:
	    </p>
	    <textarea class="form-control" rows="8" id="repl1">
	    </textarea>
	    <p>A reasonable division of the data into train, test and cross validation sets is
	      to use 60% of the data for training, and 20% each for cross validation and test sets.
	    </p>
	    <p>The data is not a completely random sample, as there is (at least) the family relationship
	      between certain individuals to consider. In order to avoid biasing our data subsets,
	      we randomise the male height data set using a shuffle before we divy it up.
	    </p>
	    <h3>Scatter Plot of data sets</h3>
	    <p>It is helpful to have a quick look at the data before running a machine learning
	      algorithm. You might learn something about the data, or find some problems of
	      missing data you weren't aware of. A scatter plot would seem reasonable for Galton's
	      data, except for the fact that we have 2 independent variables - the height of both
	      the mother and the father - so the plot would have to be 3d. I'm just going to do
	      a 2D plot using one or the other of the mother's or father's heights as the
	      independent variable.
	  </div>
	  <div class="col-md-4">
	    <h3>Galton Data - original table:</h3>
	    <div id="galtonTable" class="table dataTable">
	    </div>
	    <h3>Galton Data - Males:</h3>
	    <div id="maleTable" class="table dataTable">
	    </div>
	    <h3>Galton Data - Females:</h3>
	    <div id="femaleTable" class="table dataTable">
	    </div>	    
	  </div>
	</div> <!-- class=="row-fluid"-->
	<div class="row-fluid">
	  <!-- Add the scatter plot here. -->
	  <h1 class="indentText">Scatter Plot for Male and Female Height Data</h1>
	  <div class="col-md-4 chart1" id="mfMenu">
	    <h4>Select Dataset</h4>
	  </div>
	  <div class="col-md-8" id="mfSvg">
	  </div>
	</div>
      </div>
      <div class="container-fluid">
	<div class="row-fluid">
	  <div class="col-md-11">
            <h4>Scatter Plot observations</h4>
	    <p>From going through the different categories in the scatter plot data
	      sets, it appears that we have a reasonable decomposition of our data
	      into training, cross validation and test sets - ie. the data seems fairly
	      evenly dispursed through those sets. It also appears that there is some
	      relationship between the Father's height in inches and the child height.</p>

	    <p><em>NOTE:</em> We could also look at the relationship between maternal
	      height and child height.</p>

	    <p><h3>Data structures used</h3>
	      For our scatter plot, we used male and female datasets stored in separate objects
	      like so:
	    </p>

	    <pre class="prettyprint">
	      <code class="lang-js">
	      var maleScatter = {
                  dataName : "Male Child Height Data",
	          dataNameShort : "Child Height M",
	          dataSet : [
	              { label: "Male Training Data", d: linRegPage.maleTrain,
	                  shortLabel: "M Training"},
	              { label: "Male Cross Validation Data", d:linRegPage.maleCv,
                          shortLabel: "M Cross Validation"},
	              { label: "Male Test Data", d:linRegPage.maleTest,
	                  shortLabel: "M Test"}
	         ]
	      };
	      var femaleScatter = {
	          dataName : "Female Child Height Data",
	          dataNameShort : "Child Height F",
	          dataSet : [
	          { label: "Female Training Data", d: linRegPage.femaleTrain,
	              shortLabel: "F Training"},
	          { label: "Female Cross Validation Data", d:linRegPage.femaleCv,
	              shortLabel: "F Cross Validation"},
	          { label: "Female Test Data", d:linRegPage.femaleTest,
	              shortLabel: "F Test"}
	          ]
	      };
	      </code>
	    </pre>
	    <p> We will begin by using the maleScatter.dataSet[0].d male height training
	      data. We have the data for Father's and Mothers of Male children as an
	      array in <em>xData</em> and our male child height data in <em>yData</em>.
	      <b>Note:</b> in this case we're trying to find a predictor for child height based
	      on father and mother's height, so child height is our Y vector. <b>Note also:</b> that
	      the xData array is twice as long as the yData array, as the xData array contains
	      mother's and father's heights.
	    </p>
	    <p>
	      We start by creating <b>X</b> and <b>Y</b> vectors like this:
	    </p>
	    <pre class="prettyprint">
	      <code class="lang-js">
		var xData = [];
		var yData = [];
		
		if (childSex === "Male") {
                    yData = linRegPage.heightMale;
		    xData = linRegPage.fatherDataMale.concat(linRegPage.motherDataMale);
		}
		else {
		    yData = linRegPage.heightFemale;
		    xData = linRegPage.fatherDataFemale.concat(linRegPage.motherDataFemale);
		}

		var xRows = xData.length/2;
		var yRows = yData.length;
		assert(xRows === yRows,
		"Cannot perform linear regression with different length X and Y vectors.");
		
		// Create X as an MdArray containing a row for each of the female and male parent
		// data.
		var X = new MdArray({data: xData, shape: [2, xRows]});

		// Transpose the array to change it to having a column for the male parent heights
		// and another for the female parent heights.
		X = X.T();

		// Create Y as an xLength Md Array containing the yData.
		var Y = new MdArray({data: yData, shape: [yRows, 1]});

		// Create a new linear regression instance
		return new linReg(X, Y, "STD", 0.1, 0);
	      </code>
	    </pre>
	    <p><b>Note: </b>The MdArray and linReg classes will be discussed later. They're simple
	      implementations of multi-dimensional arrays in javascript, and linear regression using
	      MdArrays.

	      The linReg constructor above initialises various attributes of the linReg object
	      for this problem including:
	      <ul>
		<li>Set the learning rate alpha.</li>
		<li>Set the regularization parameter lambda.</li>
		<li>Normalise (or scale) the feature values in X so their values are all constrained
		  into the [-1, 1] range.</li>
		<li>Prepend a column of all 1's to the X data to allow discovery of an intercept value
		  for the linear predictor produced.</li>
		<li>Create an initial \(\theta\) vector of all 1's.</li>
	      </ul>
	      There are various ways you might choose to do normalisation on the X values before attempting
	      to run gradient descent on them. The linReg class supports STD, and MINMAX normalisation, where:

	      <ul>
		<li>STD replaces every \(X_{i}\) by \(\frac{X_{i} - \mu}{\sigma}\)</li>
		<li>MINMAX replaces every \(X_{i}\) by \(\frac{X_{i} - \mu}{max(X_{i}) - min(X_{i})}\) </li>
	      </ul>
	    </p>

	    <p><b>It's</b> interesting to note, that there might be different requirements for the
	      value of our starting \(\theta\) vector in different machine learning algorithms. For
	      example, if our matrices \(\Theta\) have the same values, ie. all ones when using
	      a neural network back propagation algorithm, the values of \(Theta\) will not be changed
	      by the gradient descent.</p>

	    <h3>Define the requirement (again):</h3>
	    <p>We want to find a vector \(\theta\) such that multiplying the vector:</p>
	    <div class="well well-sm">
	      $$ h_{\theta}(x) = \begin{bmatrix} 1 & FatherHeight & MotherHeight\\ \end{bmatrix} .
	      \begin{bmatrix}
	      \theta_{0} \\
	      \theta_{1} \\
	      \theta_{2} \\
	      \end{bmatrix}$$
	    </div>
	    <p>That is, we want a theta that allows us to make a prediction about the
	      height of a child based on the height of the child's father. In this case
	      we're limiting ourselves to looking at male children.
	    </p>

	    <h3>Function for generating a prediction.</h3>
	    <p>Our hypothesis function, \(h_{\theta}(x)\) is as follows:</p>
	    <pre class="prettyprint">
	      <code class="lang-js">
		/**
		* For each row in X, return the prediction resulting from dotting it with
		* theta.
		*
		* @param X      An MdArray containing the same number of columns as theta has
		*               rows, and containing 1 row for each example we're getting a
		*               prediction for.
		* @param theta  A theta array with the same number of rows as X has columns.
		*
		* @returns The value of X.theta.
		*/
		predict: function (X) {
		    var Xnormalised = this.normaliseVal(X).addOnes();
		    return Xnormalised.dot(this.theta);
		},		
	      </code>
	    </pre>
	    <p><b>Note</b> that unlike the formula (1) quoted above, we're not transposing the vector
	      X. This is because we've coded X as an \(n \times 3\) matrix, and \(\theta\) as a
	      \(3 \times 1\) vector, where n is the number of examples in our dataset. This means
	      that we can dot matrix \(X\) with \(\theta\) without transposing either.
	    </p>

	    <h3>Cost Function</h3>
	    <p>The cost function provides us with a measure of how close (or far away) our current \(\theta\)
	      vector is to producing a hypothesis that closely fits the data. The cost function is as
	      follows:</p>
	    <pre class="prettyprint">
	      <code class="lang-js">
		/**
		* Linear regression cost function.
		*
		* @param X        The array of features for each example.
		* @param Y        The observed values for our target variable.
		* @param theta    The current value for theta.
		*
		* @returns  The cost for the given theta.
		*/
		function costFn(X, Y, theta) {
  		    // The number of examples is the number of rows in our X or Y vectors.
		    var numExamples = X.dims[0];
		    var hypothesis = getPrediction(X, theta);
		    var diff_vector = hypothesis.sub(Y);
		    var cost_val = ((diff_vector.pow(2)).sum())/ (2 * numExamples);
		    return cost_val;
		}
		
	      </code>
	    </pre>
	    <p>The cost function can help us determine whether our regression is minimizing correctly, but for
	      the regression itself, we're more interested in an update function that uses the derivative of the
	      cost function.
	    </p>

	    <p>Our updates to theta are supplied by the function that computes:

	      <div class="well well-sm">
		$$
		\theta_{j} = \theta_{j} - \alpha \frac{1}{m}
		\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}
		$$
	      </div>
	      The function that computes the derivative of the cost function is as follows:
	    </p>
	    <p>
	      <pre class="prettyprint">
		<code class="lang-js">
	        /**
		  * Return the gradient of the cost function with respect to theta.
		  *
		  * @param X       The training "dependent" variables data set.
		  * @param Y       The training "independent" variable data set.
		  * @param theta   The current value of theta.
		  * @param alpha   The learning rate.
		  * @param lambda  The regularization parameter.
		  *
		  * @returns The gradient term for changing theta to move it towards a
		  *          minimum for the cost function.
		  */
		 function regLinGrad(X, Y, theta, alpha, lambda) {
		     var m = X.dims[0];

		     // Get the current hypothesis
		     var currentHyp = getPrediction(X, theta);

		     // Get an MdArray of the differences between currentHyp and Y.
		     var diff = currentHyp.sub(Y);

		     // Multiply the difference by X.
		     var diffByX = diff.mul(X);

		     var grad = MdArray.zeros({shape: theta.dims});
		     var firstTerm = (alpha/m) * diffByX.newSlice([":", "0"]).sum();
		     grad.set(firstTerm, 0, 0);

		     var remaining = diffByX.newSlice([":", "1:"]);
		     var mainSum = remaining.sum(1).T();
		     var lambdaTheta = theta.mul(lambda).newSlice(["1:", ":"]);
		     mainSum = (mainSum.add(lambdaTheta)).mul(alpha/m);
		     grad.slice(["1:", "0"]).assign(mainSum);
		     return grad;
		 }
		</code>
	      </pre>
	    </p>
	  </div>
	</div>
      </div>
      <div class="container-fluid">
	<div class="row-fluid">
	  <!-- Add the scatter plot here. -->
	  <h1 class="indentText">Visualising gradient descent.</h1>
	  <div class="col-md-6">
	    <p>Gradient descent works by repeatedly calculating a hypothesis value using a vector
	      \(\theta\) and updating \(\theta\) by subtracting a gradient of the cost function for
	      that \(\theta\) to move theta in a direction that minimises the cost function.
	      </p>
	    <p>
	      It is helpful to be able to visualise that process by graphing the value of the
	      cost function against the iterations of the gradient descent algorithm. As can
	      be seen, when the Gradient descent algorithm is working well, the cost should
	      reduce on each interation, until it converges to some value. The max and min
	      values of the cost function calculated for 100 iterations are shown above the
	      graph to the right.
	    </p>
	    <p>So, after 100 iterations we have a value for \(\theta\) (around [69, 0.15, -0.18] - it
	      varies as the data is shuffled differently whenever this page is loaded), and a cost
	      around about 3.5.
	      </p>
	    <h1>Evaluating the cost for our test data set.</h1>
	    <p>Previously we divided our data set up into training, cross validation and test
	      subsets. The cost for the learned \(\theta\) run on our test set is:
	    <p>
	      <div class="well well-sm" id="testCost">
		<p>
		</p>
	      </div>
	    <p>
	      Depending on our requirements, we may say that the value's we've learned using
	      gradient descent generalize quite well.
	    </p>
	  </div>
	  <div class="col-md-6" id="gradDescentSvg">
	  </div>
	</div>
      </div>
    </div>


    </div> <!-- container-fluid -->

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <!--<script src="../lib/d3.js"></script>-->
    <script data-main="../js/linearRegressionPage.js"
	    src="../bower_components/requirejs/require.js">
    </script>
  </body>
</html>
